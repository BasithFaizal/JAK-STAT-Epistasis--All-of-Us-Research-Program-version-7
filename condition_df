# Condition_df
from google.cloud import storage, bigquery
import pandas as pd
import os
import io

# Load person_ids from the parquet file in the GCS bucket
storage_client = storage.Client()
bucket_name = os.getenv("WORKSPACE_BUCKET").replace("gs://", "")
bucket = storage_client.bucket(bucket_name)
blob = bucket.blob("data/internship-directory/epistasis.parquet")

# Download and load the parquet into a DataFrame
parquet_bytes = blob.download_as_bytes()
epistasis_df = pd.read_parquet(io.BytesIO(parquet_bytes))

# Extract unique person_ids
person_ids = epistasis_df["person_id"].dropna().unique().tolist()

# Set up BigQuery client and environment
bq_client = bigquery.Client()
cdr = os.environ["WORKSPACE_CDR"]

# Build query to retrieve standard event data for filtered person_ids
query = f"""
SELECT
  person_id,
  standard_code,
  standard_vocabulary,
  standard_name,
  standard_concept_id
FROM `{cdr}.cb_review_all_events`
WHERE person_id IN UNNEST(@person_ids)
  AND standard_vocabulary IN UNNEST(@vocab_list)
"""

# Configure job with parameters
job_config = bigquery.QueryJobConfig(
    query_parameters=[
        bigquery.ArrayQueryParameter("person_ids", "INT64", person_ids),
        bigquery.ArrayQueryParameter("vocab_list", "STRING", [
            "Cancer Modifier", "ICD10CM", "ICD10PCS", "SNOMED", "HemOnc"
        ])
    ]
)

# Run the query and save the result
query_job = bq_client.query(query, job_config=job_config)
condition_df = query_job.to_dataframe()
